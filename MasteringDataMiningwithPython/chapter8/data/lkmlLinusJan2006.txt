That wasn't why I was hoping they would help..   The actual RCU callback will run uninterruptible, but I was hoping that   the patch would make it be called more often, and not have thousands of   events to work on...   Actually, I think there's a better solution.   Try just setting maxbatch back to 10 in kernel/rcupdate.c.   The thing is, "maxbatch" doesn't actually _work_ because what happens is   that the tasklet will continually re-schedule itself, and the caller will   keep calling it. So maxbatch is actually broken.   However, what happens is that after kernel/softirq.c has called the   tasklet ten times, and it is still pending, it will do the softirq in a   thread (see the "max_restart" logic).   Which happens to do the right thing, although I'm pretty convinced that it   was by mistake (or if on purpose, it depends way too closely on silly   magic implementation issues).   I raised maxbatch because the dentry RCU problem (since fixed) was   reported to become less (probably because the scheduling of the softirq's   delayed the freeing of old dentries further, making the problem worse),   and because I don't like the subtlety involved in having two layers of   batching. But hey, it might work, at which point we should just document   how maxbatch and max_restart interact with each other and with softirqd.   		 
NO IT HAS NOT.   This is total revisionist history by gcc people. It did not use to be a   hint. If you asked for inlining, you got it unless there was some   technical reason why gcc couldn't inline it (ie recursive inlining, and   trampolines and some other issues). End of story.    So don't fall for the "hint" argument. It's simply not true.   At some point during gcc-3.1, gcc people changed it to be a hint, and did   so very badly indeed, where functions that really needed inlining (because   constant propagation made them go away) were not inlined any more. As a   result, we do   in linux/compiler-gcc3.h exactly because gcc-3.1 broke so badly.   And nobody sane will argue that we would _ever_ not do that. gcc-3 was   just too broken. Some architectures (sparc64, MIPS, s390) ended up trying   to tune the inlining limits by hand (usually making them effectively   infinitely large), but the basic rule was that gcc-3 inlining was just not   working.   It may have improved in later gcc-3 versions, and apparently it's getting   better in gcc-4. And that's the only thing we're discussing: whether to   let gcc _4_ finally make some inlining decisions.   And people are nervous about it, exactly because the gcc people have   historically just changed what "inline" means, with little regard for   real-life code that uses it. And then they have this revisionist agenda,   trying to change history and claiming that it's always been "just a hint".   Despite the fact that the gcc manual itself very much said otherwise (I'm   sure the manuals have been changed too).   		 
That's actually not a good practice. Two reasons:    - debuggability goes way down. Oops reports give a much nicer call-chain      and better locality for uninlined code.    - Gcc can suck at big functions with lots of local variables. A      function call can be _cheaper_ than trying to inline a function,      regardless of whether it's called once or many times. I've seen      functions that had several silly (and unnecessary) spills suddenly      become quite readable when they were separate functions.      More importantly, the "inline" sticks around. Later on, the function is      used for some other place too, and the inline doesn't get removed.   The second "the inline sticks around" case is something that happens to   helper functions too. They started out as trivial macros in a header file,   but then they get converted to an inline function because they get more   complex, and eventually it grows a new hook. Suddenly what used to   generate two instructions generates ten instructions and a call, and would   have been much better off being uninlined in a .c file.   So inlines that make sense at one point have a tendency to _not_ make   sense a year or two later.    I suspect we'd be best off removing almost all inlines, both from C files   and headers. There are a few cases where inlining really helps: the   function really ends up being just a few instructions, and it's really   just a wrapper around a simpler calling convention or an inline assembly,   or it's called with constant arguments that are better left off simplified   at compile-time. Those are the cases where inlining really helps.   (Of course, then there's ia64. Which wants inlining just because..)   		 
This is totally not relevant.   99% of all bug-reports happen for non-developers. What developers can and   can not do from a debuggability standpoint is almost totally   uninteresting: quite often the developers won't even be able to recreate   the bug, but have to go on the bug report that comes in from the outside.   And yes, some users are willing to recompile the kernel, and try ten    But many people have trouble even reporting the (short) oops details, much   less follow up on it.    So it's actually important that the default config is reasonably easy to   debug from the oops report. Because it may be the only thing you ever get.   So -O0 and -fno-inline simply isn't practical, because they are not an   option for a normal kernel.    		 
Ok, very small/few changes in the last week, it seems everybody is off on   vacation. All the better.   The shortlog/diffstat tell the story: a few one-liners, another ipv6   deadlock fixed, some sysctl and /proc fixes.    Have fun,   		 
Note that semaphores are definitely used from interrupt context, and as   such you can't replace them with mutexes if you do this.   The prime example is the console semaphore. See kernel/printk.c, look for   "down_trylock()", and realize that they are all about interrupts.   			 
Ok, this patch looks ok, if it's confirmed to unbreak apache.   		 
Actually, looking closer, this patch does the wrong thing for a size_t   that is negative in ssize_t (which is technically "undefined behaviour" in   POSIX, but turning it into a big positive number is objectively worse than   returning -EINVAL).   		 
Just {read|write}[v] and sendfile. With compat32 functions (although   those, for obvious reasons, have just 32-bit counts anyway, so they have a   natural limit from that).   (it will go through the network layer), so they don't have the same "limit   damage from untrusted code" issues.   So here's a suggested patch.   It changes "rw_verify_area()" in a simple way:    - it continues to return -EINVAL for truly invalid sizes (ie if the      "size_t" is negative in "ssize_t")   - it returns the _truncated_ size for other cases (ie it will now return      a non-negative "suggested count" for success)   this means that the "hard limit" of INT_MAX is gone, and it's up to the   caller to decide how it reacts to a positive value. A caller can choose to   just ignore it ("I'll use my untruncated count, thank you") or can choose   to take the truncated value.   In the first case, the only change is that instead of checking the return   value against zero (any non-zero being error), the caller has to realize   that positive values aren't error cases. So   	ret = rw_verify_area(...)   turns into   	ret = rw_verify_area(...)   and in the second case, the caller just ends up adding a "count = ret"   afterwards.   The rw_verify_area() function isn't exported to modules, and is used in   only a few places, so this all seems pretty straightforward.   Untested, but mostly sane-looking patch appended. I do want to do   something about the "readv|writev()" case, though, so it's not complete (I   don't want to have readv/writev pass insane huge values down the stack to   untrustworthy drivers..)   Comments? Can somebody test this with the apache thing that caused this   to surface?   (And I'd like to stress that this really is untested. I've not even   compile-tested it, so while it all looks very straightforward, typos and   thinkos can abound)   		 
One of the problems with code size optimizations is that they   fundamentally show much less impact under pretty much any traditional   benchmark.   In user space, for example, the biggest impact of size optimization tends   to be in things like load time and low-memory situations. Yet, that's   never what is benchmarked (yeah, people will benchmark some low-memory   kernel situations by compating different swap-out algorithms etc against   each other - but they'll almost never compare the perceived speed of your   desktop when you start swapping).   Now, I'll happily also admit that code and data _layout_ is often a lot   more effective than just code size optimizations. That's especially true   with low-memory situations where the page size being larger than many of   the code sequences, you can make a bigger impact by changing utilization   than by changing absolute size.   But even then it's actually really hard to measure. Cache effects tend to   be hard _anyway_ to measure, it's really hard when the interesting case is   the cold-cache case and can't even do simple microbenchmarks that repeat a   million times to get stable results.   So the best we can usually do is "microbenchmarks don't show any   noticeable _worse_ behaviour, and code size went down by 10%".   Just as an example: there's an old paper on the impact of the OS design on   the memory subsystem, and one of the tables is about how many cycles per   instruction is spent on cache effects (this was Ultrix vs Mach, the load   was the average of a workload that was either specint or looked a lot   like it).   			I$	D$   	Ultrix user:	0.07	0.08  	Mach user:	0.07	0.08  	Ultrix system:	0.43	0.23  	Mach system:	0.57	0.29   Now, that's an oldish paper, and we really don't care about Ultrix vs Mach   nor about the particular hw (Decstation), but the same kind of basic   numbers have been repeated over an over. Namely that system code tends to   have very different behaviour from user code wrt caches. Caches may have   gotten better, but code has gotten bigger, and cores have gotten faster.   And in particular, they tend to be _much_ worse. Something you'll seldom   see as clearly in micro-benchmarks, if only because under those benchmarks   things will generally be more cached - especially on the I$ side.   So I should probably try to find something slightly more modern, but what   it boils down to is that at least one well-cited paper that is fairly easy   to find claims that about _half_a_cycle_ for each instruction was spent on   I$ misses in system code on a perfectly regular mix of programs. And that   the cost of I$ was actually higher than the cost of D$ misses.   Now, most of the _time_ tends to be spent in user mode. That is probably   part of the reason for why system mode gets higher cache misses (another   is that obviously you'd hope that the user program can optimize for its   particular memory usage, while the kernel can't). But the result remains:   I$ miss time is a noticeable portion of system time.   Now, I claim that I$ miss overhead on a system would probably tend to have   a pretty linear relationship with the size of the static code. There   aren't that many big hot loops or small code that fits in the I$ to skew   that trivial rule.   So at a total guess, and taking the above numbers (that are questionable,   but hey, they should be ok as a starting point for WAGging), reducing code   size by 10% should give about 0.007 cycles per instruction in user space.   Whee. Not very noticeable. But on system code (the only thing the kernel   can help), it's actually a much more visible 0.05 CPI.   Yeah, the math is bogus, the numbers may be irrelevant, but it does show   why I$ should matter, even though it's much harder to measure.   			 
If Christoph is happy with it, and there has been no grumbling from -mm, I   can certainly merge it.   However, I really _really_ prefer that people who use git to merge use the   native git protocol, which I trust. That http: thing may work, but it's a   cludge ;)   Can you run git-daemon on the machine?    		 
Strange, because it doesn't merge with your other own changes. It might be   an ordering thing (ie they might have merged fine in another order). Or   maybe it's just because the -mm scripts will force-apply patches (or drop   them).   Anyway, there were clashes in drivers/usb/core/usb.c with the patch "USB:   fix usb_find_interface for ppc64" that came through your USB changes, and   that gets a merge error with the uevent/hotplug thing.   I can do the trivial manual fixup, but when I do, I have two copies of   "usb_match_id()": one in drivers/usb/core/driver.c and one in   drivers/usb/core/usb.c.   I've pushed out my tree, so that you can see for yourself (it seems to   have mirrored out too).   		 
Actually, looking closer, my first trivial merge was wrong (it took the   code from both branches), and doing it right seems to get the proper   results (with just one usb_match_id() function).   I'll push out my _proper_ trivial merge fixup, please just verify that the   end result looks sane and matches what you have.   		 
Well, if the generic one generates _buggy_ code on ppc64, that means that   either the generic version is buggy, or one of the atomics that it uses is   buggily implemented on ppc64.   And I think it's the generic mutex stuff that is buggy. It seems to assume   memory barriers that aren't valid to assume.   A mutex is more than just updating the mutex count properly. You also have   to have the proper memory barriers there to make sure that the things that   the mutex _protects_ actually stay inside the mutex.   So while a ppc64-optimized mutex is probably a good idea per se, I think   the generic mutex code had better be fixed first and regardless of any   optimized version.   On x86/x86-64, the locked instructions automatically imply the proper   memory barriers, but that was just lucky, I think.   			 
Goodie.   Yeah, I "forgot" to start that particular flame-war.   Ok, here's the last chance for people to complain. If I don't get any   complaints, I'll "remember" to pull your tree.   			 
Ok, fair enough. However, that still leaves the question of which way the   barrier works. Traditionally, we have only cared about one thing: that all   preceding writes have finished, because the "atomic_dec_return" thing is   used as a _reference_counter_, and we're going to release the thing.   However, that's not the case in a mutex. A mutex locking operation works   exactly the other way around: it doesn't really care about the previous   writes at all, since those operations were unlocked. It cares about the   _subsequent_ writes, since those have to be seen by others as being in the   critical region, and never be seen as happening before the lock.   So I _think_ your argument is bogus, and your patch is bogus. The use of   "atomic_dec_return()" in a mutex is _not_ the same barrier as using it for   reference counting. Not at all. Memory barriers aren't just one thing:   they are semi-permeable things in two different directions and with two    So I think the barrier has to come _after_ the atomic decrement (or   exchange).    Because as it is written now, any writes in the locked region could   percolate up to just before the atomic dec - ie _outside_ the region.   Which is against the whole point of a lock - it would allow another CPU to   see the write even before it sees that the lock was successful, as far as  I can tell.   But memory ordering is subtle, so maybe I'm missing something..   		 
Yes. And it's nice even when unconditional branches are effectively free,   because it can avoid an unnecessary cache miss just to fetch the   unnecessary branch (which very much _can_ happen, since the failure   function will sleep).   Of course, the thing to look out for is to never get the call-return stack   messed up, but this kind of regular tail-call doesn't have that issue.   			 
If it causes the conditional jump to become a long one instead of a byte   offset one, it's actually a pessimisation for no gain (yes, it might give   better cache density _if_ the function that is linked after the current   one is cache-dense with the function in question and _if_ the unlikely   sequence is really really unlikely, but that's two fairly big ifs).   So I'm not at all convinced of the feature (or maybe gcc actually does the   right thing, and the reason you can't reproduce it is because gcc is being   understandably reluctant to use the other section).   Basically, there's "biased one way", "unlikely", and there's "practically   never happens". And even the "practically never" case will probably be   better off with the unlikely case close-by, if it means that the likely   case can use a short branch.   			 
The fact is, the last thing we want to do is to ship a magic profile file   around for each and every release. And that's what we'd have to do to  get consistent and _useful_ performance increases.   That kind of profile-directed stuff is useful mainly for commercial binary   releases (where the release binary can be guided by a profile file), or   speciality programs that can tune themselves a few times before running.   A kernel that people recompile themselves simply isn't something where it   works.   What _would_ work is something that actually CHECKS (and suggests) the   hints we already have in the kernel. IOW, you could have an automated   test-bed that runs some reasonable load, and then verifies whether there   are branches that go only one way that could be annotated as such, or   whether some annotation is wrong.   That way the "profile data" actually follows the source code, and is thus   actually relevant to an open-source project. Because we do _not_ start   having specially optimized binaries. That's against the whole point of   being open source and trying to get users to get more deeply involved with   the project.   			 
Btw, having annotations obviously works, although it equally obviously   will limit the scope of this kind of profile data. You won't get the same   kind of granularity, and you'd only do the annotations for cases that end   up being very clear-cut. But having an automated feedback cycle for adding   (and removing!) annotations should make it pretty maintainable in the long   run, although the initial annotations migh only end up being for really   core code.   There's a few papers around that claim that programmers are often very   wrong when they estimate probabilities for different code-paths, and that   you absolutely need automation to get it right. I believe them. But the   fact that you need automation doesn't automatically mean that you should   feed the compiler a profile-data-blob.   You can definitely automate this on a source level too, the same way   sparse annotations can help find user access problems.    There's a nice secondary advantage to source code annotations that are   actively checked: they focus the programmers themselves on the issue. One   of the biggest advantages (in my opinion) of the "struct xyzzy __user *"   annotations has actually been that it's much more immediately clear to the   kernel programmer that it's a user pointer. Many of the bugs we had were   just the stupid unnecessary ones because it wasn't always obvious.   The same is likely true of rare functions etc. A function that is marked   "rare" as a hint to the compiler to put it into another segment (and   perhaps optimize more aggressively for size etc rather than performance)   is also a big hint to a programmer that he shouldn't care. On the other   hand, if some branch is marked as "always()", that also tells the   programmer something real.   So explicit source hints may be more work, but they have tons of   advantages. Ranging from repeatability and distribution to just the   programmer being aware of them.   In other projects, maybe people don't care as much about the programmer   being aware of what's going on - garbage collection etc silent automation   is all wonderful. In the kernel, I'd rather have people be aware of what   happens.   		 
Yes, I think "crude" is exactly where we want to start. It's much easier   to then make it smarter later.   Yes. And it's entirely possible that "crude" is perfectly fine even in the   long run. I suspect this is very much a "5% of the work gets us 80% of the   benefit", with a _very_ long tail with tons of more work to get very minor   incremental savings..   The cache effects are likely the biggest ones, and no, I don't know how   much denser it will be in the cache. Especially with a 64-byte one..   (although 128 bytes is fairly common too).   There are some situations where we have TLB issues, but those are likely   cases where we don't care about placement performance anyway (ie they'd  be in situations where you use the page-alloc-debug stuff, which is very   expensive for _other_ reasons ;)   		 
Oh, but validatign things like "likely()" and "unlikely()" branch hints   might be a noticeably bigger issue.    In user space, placement on the macro level is probably a bigger deal, but   in the kernel we probably care mostly about just single cachelines and   about branch prediction/placement.   		 
Yeah, don't trash your backups.   If/when you try something again, how about these two trivial one-liners?   I'm not 100% sure the mapcount sanity check is strictly speaking right (no   locking between mapcount/pagecount comparison), but the page count really   should never fall below the mapcount, so aside from races that I don't   think can be triggered in practice, this might be very useful to find   where those pesky page counts suddenly disappear..   Right now we get the oops "too late" - something has decremented the page  count way too far, but we don't know what it was, and the actual function   that triggers it _seems_ to be harmless.   The other part of the patch is to clear "page" when it's being free'd, in   case somebody tries to free the same thing twice. I don't see how that   could happen either, but...   The patch is untested in every way. No guarantees.   		 
I don't believe it is actually all _that_ volatile. Yes, it would be a   huge issue _initially_, but the incremental effects shouldn't be that big,   or there is something wrong with the approach.   I suspect that would certainlty work for pure function-based popularity,   and yes, it has the advantage of being simple (especially for something   that ends up being almost totally separated from the compiler: if we're   using this purely to modify link scripts etc with special tools).   But what about the unlikely/likely conditional hints that we currently do   by hand? How are you going to sanely maintain a list of those without   doing that in source code?   			 
Yes.   No. The alternative is to just make the ordering requirements   for "atomic_dec_return()" and "atomic_xchg()" be absolute. Say that they   have to be full memory barriers, and push the problem into the low-level  architecture.   I _think_ your patch is the right approach, because most architectures are   likely to do their own fast-paths for mutexes, and as such the generic   ones are more of a template for how to do it, and hopefilly aren't that   performance critical.   			 
Shouldn't you make that "isync" dependent on SMP too? UP doesn't need it,   since DMA will never matter, and interrupts are precise.   		 
Yes, but we only do this for routines that we look at anyway.   Also, the profiles can be misleading at times: you often get instructions   with zero hits, because they always schedule together with another   instruction. So parsing things and then matching them up (correctly) with   the source code in order to annotate them is probably pretty nontrivial.   But starting with the code-paths that get literally zero profile events is   definitely the way to go.   We don't have likely()/unlikely() that often, and at least in my case it's   partly because the syntax is a pain (it would probably have been better to   include the "if ()" part in the syntax - the millions of parenthesis just   drive me wild).   So yeah, we tend to put likely/unlikely only on really obvious stuff, and   only on functions where we think about it. So we probably don't get it   wrong that often.   That's an interesting use of "fortunately". I tend to prefer the form   where it means "fortunately, we can trivially fix this with a two-line   solution that is obviously correct" ;)   		 
Ehh. What's a "quick glance" to a human can be quite hard to automate.   That's my point.   If we do the "human quick glances", we won't be seeing much come out of   this. That's what we've already been doing, for several years.   I thought the discussion was about trying to automate this..   		 
That warning is totally bogus. It shouldn't be printed out at all when   "newirq" is 0 (as in this case).    Even for a non-zero newirq, I suspect that 99% of the time,   "pci=usepirqmask" would end up causing more problems than it could ever   solve.   But this diff would seem to be the minimal fix.   The other problems _look_ like they are -mm related, not in plain 2.6.15.   Etienne, can you confirm?   			 
Don't use this one. It doesn't zero-pad the string if the result buffer is   too small (which _could_ cause problems) and it actually returns the wrong   length too.    There's a better one in the final 2.6.15.   			 
That's just SIGKILL (128+9). Which is normal for the OOM killer.   Well, sparse does keep a _lot_ of stuff in memory, and the "do many files   at once" will basically keep every single one (with full types, full   linearization etc) in memory at the same time.   It's probably fairly easy to fix: I should just make sparse release all   the linearizations and symbols when they go out of file scope.   The "do many files at once" thing really was just a quick hack, so the   lack of memory release is not that susprising.   I'll see what I can do.   		 
Len,   I _really_ wish you wouldn't have those automatic merges.   Why do you do them? They add nothing but ugly and unnecessary history, and   in this pull, I think almost exactly half of the commits were just these   empty merges.   There's just no point, except to make the history harder to read.   So please stop it. You have some of the ugliest history around, and it's   all just because you have some automated process that merges unnecessarily   all the time.   If you do merges because you want to _test_ the development with a merged   tree, that doesn't have to happen in the development branch itself. Nobody   else cares about such a merge except the tester (unless the test fails of   course, and you need to fix up the result - at which point it's not an   automated merge any more).   		 
I think you should make the error messages a bit more informative. Like   saying   	You need to build the tree before you can install it   instead of saying "missing file: xyzzy".   		 
That's not the point. It does: "git log --no-merges" does exactly that.   But fire up "gitk" to watch the history, and see the difference.   That has absolutely nothing to do with anything. It's not the comment   (which admittedly gives absolutely no information - but why should it,   since the _commit_ itself has no information in it?)   It's like you have empty commits that don't do anything at all, except   that they are worse, because they have two parents.   No. Your commits make it harder for _everybody_ to track the history.    A merge by definition "couples" the history of two branches. That's what a   merge very fundamentally is. It ties two things together. But two things   that don't have any connection to each other _shouldn't_ be tied together.   Just as an example: because of the extra merges, you've made all your   commits dependent on what happened in my tree, with no real reason. So   let's say that somebody reports that something broke in ACPI. Now you   can't just go to the top of the ACPI history and work backwards - you'll   have tied up the two histories so that they are intertwined.   And yes, you can always work around it, but there's just no point. And   none of the other developers seem to need to do it. They do their   development, and then they say "please pull". At that point the two   histories are tied together, but now they are tied together for a   _reason_. It was an intentional synchronization point.   An "automated pull" by definition has no reason. If it works automated,   then the merge has zero semantic meaning.    			 
They DO.   Len, you're doing EXTRA WORK that is pointless.   Just stop doing the automated merges. Problems solved. It really is that   easy. Don't do what David suggests - he does it because he's apparently   _so_ comfortable with things that he prefers to do extra work just to keep   his trees extra clean (I actually would disagree - but git makes that   fairly easy to do, so if you prefer to have as linear a history as   possible, you can do it with git pretty easily).   Now, I'm only complaining about _automated_ merges. If you have a reason   to worry about my tree having clashes with your tree, do a real merge. For   example, in your latest pull, you had a    	"pull   into release branch"
Yes, it's fairly easy to do. That said, I would actually discourage it. I   haven't said anything to David, because he is obviously very comfy with   the git usage, and it _does_ result in cleaner trees, so especially since   the networking code ends up being the source of a lot of changes, the   extra cleanup stage that David does might actually be worth it for that   case.   But git is actually designed to have parallel development, and what David   does is to basically artificially linearize it. We merge between us often   enough that it doesn't really end up losing any historical information   (since David can't linearize the stuff that we already merged), but in   _theory_ what David does actually does remove the historical context.   So "git-rebase" is a tool that is designed to allow maintainers to (as the   command says) rebase their own development and re-linearize it, so that   they don't see the real history. It's basically the reverse of what Len is   doing - Len mixes up his history with other peoples history in order to   keep them in sync, while David bassically "re-does" his history to be on   top of mine (to keep it _separate_).   The "git-rebase" means that David will always see the development he has   done/merged as being "on top" of whatever my most recent tree is. It's   actually a bit scary, because if something goes wrong when David re-bases   things, he'll have to clean things up by hand, and git won't help him   much, but hey, it works for him because (a) things seldom go wrong and (b)   he appears so comfortable with the tool that he _can_ fix things up when   they do go wrong.   And yes, git-rebase can be very convenient. It has some problems too   (which is the other reason I don't try to convince other maintainers to   use it): because it re-writes history, a change that _might_ have worked   in its original place in history might no longer work after a rebase if it   depended on something subtle that used to be true but no longer is in the   new place that it has been rebased to.   Which just means that a commit that was tested and found to be working   might suddenly not work any more, which can be very surprising ("But I   didn't change anything!").   On the other hand, this is no different from doing a merge of two   independent streams of development, and getting a new bug that didn't   exist in either of the two, just because they changed the assumptions of   each other (ie not a _mismerge_, but simply two developers changing   something that the other depended on it, and the bug only appears when   both the working trees are merged and the end result no longer works).   So my suggested git usage is to _not_ play games. Neither do too-frequent  merges _nor_ play games with git-rebase.   That said, git-rebase (and associated tools like "git-cherry-pick" etc)   can be a very powerful tool, especially if you've screwed something up,   and want to clean things up. Re-doing history because you realized that a   you did something stupid that you don't want to admit to anybody else.   So trying out git-rebase and git-cherry-pick just in case you decide to   want to use them might be worthwhile. Making it part of your daily routine   like David has done? Somewhat questionable, but hey, it seems to be   working for David, and it does make some things much easier, so..   			 
Right. You have to do one or the other (rebase your changes to another   tree _or_ merge another tree into your changes), but not mix the two.   		 
This part looks bogus:   That rule should probably be:   	fdimage fdimage144 fdimage288: vmlinux  	install:  		$(Q)$(MAKE) $(build)=$(boot) BOOTIMAGE=$(BOOTIMAGE) $@   since the fdimages should still depend on vmlinux.   No?   		 
No.   If tree B is based on _some_point_in_ A, then you just test that.   Because development line B is _independent_ of development line A. The   fact that A changes doesn't change B - unless they have some real   dependencies (which we should try to avoid).   So when you update ("fetch" in git parlance) branch A from me, that   shouldn't affect branch B _nor_ branch C in any way. They clearly do not   depend on the new stuff in A, since they do their own independent   development. The fact that they _started_ at some random point during the   development of A doesn't change that fact.   Now, if you want to _test_ the combined "new stuff in branch A and new   stuff in branch B", feel free to do that. But realize that that is _not_   appropriate in either branch A _nor_ branch B.   So you'd be much better off with a separate "test" branch that you test   stuff out in, and you then resolve ("pull" in git parlance) both branch A   and branch B into that test branch.   See? Testing the combination of two branches doesn't actually have   anything to do with either branch.   At some point, you decide that you want to merge what you've done in   branch B. That's a _different_ and independent thing from deciding that   you want to test the combination of two development branches. Clearly,   it's great to test often, but that has nothing to do with releasing a   branch.   I'm saying that mixing up the "let's test the combination" and "let's   merge the two branches" are totally different things and should not be   mixed up.   One is a random event (and then it makes sense to have, for example, a   "automated test branch" that automatically merges every day and tests the   results. I don't think you should expose those random merges to others,   because they actually hinder the readability of the history for _both_   sides.   The other is a _directed_ event. It's the event of saying "branch B" is   now ready to be merged. Usually that's best done by just saying "please   pull now" - ie not by merging branch A into branch B (because that's not   what you actually want, is it? What you want is for the development in   branch B to show up in branch A - so you want branch A to do the pull).   Now, there's a third kind of event, which is again independent of the   other two. It's more of a "let's try to keep the 'topic branch'   development up-to-date with the branch we eventually want to merge the   topic changes into". That's where you can now do two things:    - David often "rebases" all of the changes in his "topic branch" (ie      conceptually "branch B") to the new top-of-head of "branch A". In other      words, he re-writes branch B entirely _as_if_ it was based on the newer      state "branch A". This is what "git rebase" is all about.    - You can just pull from branch A into branch B, as a way to keep branch      B more up-to-date with the work in the "main trunk" or whatever. This      is ok, but it shouldn't be a common event. It should be something that      happens when you (for example) notice during testing that the test      merge no longer works cleanly. Or it might be "It's now been two weeks      since I synchronized, let's just synchronize to be safe".   See? I'm not objecting to topic branches pulling from my tree in general.   It's just that they should have a _reason_. There's never any reason to   pull into a development tree that you haven't done any development in,   just because you also want to use that development tree for testing.   That's one way. It's often the best way, especially if it's a really   obvious bugfix. Or you could just fix it in your tree yourself. It will   mean that the two branches have the same fix, but especially if it really   is an identical fix, it won't be a merge problem.   You _can_ just decide to pull branch B into branch C, but that has a real   problem, namely that it inexorably links the two together, so that nobody   can then pull branch C without pulling indirectly branch B at the time   that B-C merge happened. Sometimes that is ok. But it's nice to avoid it   if you can.   But for example, if somebody fixed something in the trunk, and you   actually do need that fix from the trunk for your topic branch   development, then just doing a pull is _fine_. Now we're back to doing a   merge that actually has a perfectly good reason.   IOW, don't cherry-pick to avoid merges when the merge really does make   tons of sense. Merges are good, it's just that _too_ much of a good thing   is bad.   No, all the good tools really are based on fetching (NOT "pulling") the   other branch into your local tree as a separate branch. At that point,   there are tons of wonderful tools you can use.   In other words, say that you want to know what has happened in another   repository, at git://git.kernel.org/xyzzy. You aren't interested in the   stuff that is already part of the trunk, you're just interested in what is   only in that "xyzzy" branch, and how it relates to your code.    What you'd do is   	git fetch git://git.kernel.org/xyzzy master:xyzzy-snapshot   which says "fetch the 'master' branch from that xyzzy repository, and call   it 'xyzzy-snapshot' locally.   You can then (for example) fetch the code that is in _my_ tree by doign   the same time (just call that branch ' '), and you can now do
I'd do   	git fetch  
Actually, these days git is pretty good at it. Much better than CVS,   certainly. You can see the "conflict against my old tree", or "conflict   against the remote tree" by using the "--ours" or "--theirs" flag to "git    (Or "diff conflict against common base": "git diff --base").   So for your particular example with a trivial base file:   	line    1  	        2  	        3   	                bar  	        6  	        7  	        8   and then the changes you had as an example in the A' and B branches, if I   from A' do a "git pull . B", I get:   	Trying really trivial in-index merge...  	fatal: Merge requires file-level merging  	Nope.  	Merging HEAD with ad56343c578785b8d932224a8676615e7a3e191f  	Merging:   	9d619225e3adecee6432a36d67d140e29b0acf62 A' case   	ad56343c578785b8d932224a8676615e7a3e191f B: case   	found 1 common ancestor(s):   	93765ba3f64e9c73438e52683fffa68e5a493df7 Base commit   	Auto-merging A   	CONFLICT (content): Merge conflict in A    	Automatic merge failed; fix up by hand   and then the file contains the contents   	line    1  	        2  	        3  	 HEAD/A   	=======   	 ad56343c578785b8d932224a8676615e7a3e191f/A  	                bar  	        6   	        7  	        8   ie it will have does a CVS-like merge for me, and I need to fix this up.   However, to _help_ me fix it up, I can now see what the diff is aganst my   original version (A'), with "git diff --ours" (the "--ours" is default, so   it's unnecessary, but just to make it explicit):   	 line   1  	        2  	        3   	                bar  	        6   which is very helpful especially once I have resolved it. IOW, I just edit   the file and do the trivial resolve, and now I can do a "git diff" again   to make sure that it looks ok:   	        2  	        3   	                bar  	        6   ahh, looks good, so I just do "git commit A" and that creates the   resolved merge.   Now, this is a real issue.    The resolve part is pretty easy, but the fact that it's hard to see in   "git-whatchanged" is a limitation of git-whatchanged.    You need to use "gitk", which _does_ know how to show merges as a diff   (and yes, I just checked).   You're just missing the fact that git-whatchanged (or rather,   "git-diff-tree") isn't smart enough to show merges nicely. It really   _should_. It doesn't. You can choose to show merges with the "-m" flag,   but that will show diffs against each parent, which really isn't what you   want.    I should do the same thing gitk does in git-diff-tree.   Now, git-diff-tree _does_ do that. Use the "-m" flag, and choose the tree   you want.   And btw, that works with "git-whatchanged" too. You _can_ pass the "-m"   flag to git-whatchanged, and it will show you each side of the merge   correctly. So it _works_. It's just such a horrible format that by default   it prefers to shut up about merges entirely.    (I don't know of a good three-way diff format. "gitk" can do it, because   gitk can show colors. That's a big deal when you do three-way - or   more-way - diffs).   You do _not_ want to rebase a merge. It not only won't work, it's against   the whole point of rebasing.   Rebasing is really only a valid operation when you have a few patches OF   YOUR OWN that you want to move up to a new version of somebody elses tree   that you are tracking. You fundamentally _cannot_ rebase if you've done   anything but a linear set of patches. And that has nothing to do with the   patch difficulty - it simply isn't an operation that makes sense.   (Btw, not making sense doesn't mean it might not work. It sometimes might   actually work and do what you _hoped_ it would do, but it's basically by   pure luck, and not because it is a sensible operation. Even stupid   people hit on the right solution every once in a while - not because   they thought about things right, but just because they happened to try   something that worked. The same is true of "git rebase" with merges ;^).   The fundamental reason a rebase doesn't make sense is that if you've done   a merge, it obviously means that some other branch has done development,   and already has the commits that you're trying to rebase. And you CANNOT   rebase for them.   So instead of rebasing across a merge, what you can do is to not do the   merge at all, but instead rebase one of the two branches against the   other. Then you can rebase the result against the thing that you wanted to   rebase them both against. Now you've never rebased a merge - you've just  linearised branches that were linear in themselves against each other.   Basically, a merge ties two branches together. Once you've merged, you   can't make a linear history any more. The merge fundamentally is not   linear.   		 
Well, it sure triggered.   and it's that same sgl_unmap_user_pages() that keeps on triggering it.   Which was not what I was hoping for. I was hoping we'd see somebody _else_   decrementing the page count below the map count, and get a new clue.   notice of something. That's "dirty", and maybe it's from   in that same sgl_unmap_user_pages() routine.. And it strikes me that that   is bogus.   Code like that should use "set_page_dirty()", which does the appropriate   callbacks to the filesystem for that page. I wonder if the bug is simply   because the ST code just sets the dirty bit without telling anybody else   about it...   Gaah. Hugh, Nick?   		 
Paul,    RCU patches always make me worry, so can you please Ack or Nack this   series?   		 
To be fair, backtracking a "git-rebase" isn't obvious. One of the   downsides of rebasing.   		 
Well, as I warned in the message that had the patch, the test _is_ racy.   The reads of the page counts have no serialization, so if another process   is changing them, I don't guarantee that the test is correct.   IOW, it was meant as a special case debug test for one particular problem   where I hoped it would give more information, rather than a real patch.   		 
One thing we could do is to make it easier to apply a patch to a   _non_current_ branch.   In other words, let's say that we want to encourage the separation of a   "development branch" and a "testing and use" branch (which I'd definitely   personally like to encourage people to do).   And one way to do that might be to teach "git-apply" to apply patches to a   non-active branch, and then you keep the "testing and use" branch as your   _checked_out_ branch (and it's going to be really dirty), but when you   actually apply patches you could do that to the "development" branch with   something like   	git-apply -b development  patch-file   (Now, of course, that's only if you apply somebody elses patch - if you   actually do development _yourself_, you'd either have to check out the   development branch and do it there, or you'd move the patch you have in   your "ugly" checked-out testing branch into the development branch with   	git diff | git-apply -b development   or something similar..)   Then you could always do "git pull . development" to pull in the   development stuff into your working branch - keeping the development   branch clean all the time.   Do you think that kind of workflow would be more palatable to you? It   shouldn't be /that/ hard to make git-apply branch-aware... (It was part of   my original plan, but it is more work than just using the working   directory, so I never finished the thought).   "gitk" is actually pretty good at showing multiple branches. Try doing a   	gitk --all -d   and you'll see all the topic branches in date order. The "-d" isn't   strictly necessary, and to some degree makes the output messier by   interleaving the commits from different branches, so you may not want to   do it, but it is sometimes nice to see the "relative dates" of individual   commits rather than the denser format that gitk defaults to.   Yes, topic branches will always imply more commits, but I think they are   of the "nice" kind.   I definitely encourage people to use git as a distributed concurrent   development system ratehr than the "collection of patches" thing. Quilt is   much better at the collection of patches.    So I'd encourage topic branches - even within something like ACPI, you   might have separate topics ("interpreter" branch vs "x86" branch vs   "generic-acpi" branch).   And yes, that will make history sometimes messier too, and it will cause   more merges, but the difference there is that the merges will be   meaningful (ie merging the "acpi interpreter" branch into the generic ACPI   branch suddenly has _meaning_, even if there only ends up being a couple   of commits per merge).   Ok?   		 
Btw, this is true in a bigger sense: the things "git" does have largely   been driven by user needs. Initially mainly mine, but things like   "git-rebase" were from people who wanted to work as "sub-maintainers" (eg   Junio before he became the head honcho for git itself).   But if there are workflow problems, let's try to fix them. The "apply   patches directly to another branch" suggestion may not be sane (maybe it's   too confusing to apply a patch and not actually see it in the working   tree), but workflow suggestions in general are appreciated.   We've made switching branches about as efficient as it can be (but if the    never going to be low). But switching branches has the "confusion factor"   (ie you forget which branch you're on, and apply a patch to your working   branch instead of your development branch), so maybe there are other ways   of doing the same thing that might be sensible..   So send suggestions to the git lists. Maybe they're insane and can't be   done, but while I designed git to work with _my_ case (ie mostly merging   tons of different trees and then having occasional big batches of   patches), it's certainly _supposed_ to support other maintainers too..   		 
Yes. It has many advantages, and it's the approach I pushed pretty hard   originally, but the "many branches in the same tree" approach seems to   have become the more common one. Using many branches in the same tree is   definitely the better approach for _distribution_, but that doesn't   necessarily mean that it's the better one for development.   For example, you can have a git distribution tree with 20 different   branches on kernel.org, but do development in 20 different trees with just   one branch active - and when you do a "git push" to push out your branch   in your development tree, it just updates that one branch on the   distribution site.   So git certainly supports that kind of behaviour, but nobody I know   actually does it that way (not even me, but since I tend to just merge   other peoples code, I don't actually have multiple branches: I create   temporary branches for one-off things, but don't maintain them that way).   			 
Before I pull this, I'd like to get some confirmation that some of the   other problems that seem to be PCI-related in the -mm tree are also   understood, or at least known to be part of the stuff that you're _not_   sending me..   [ There's at least a pci_call_probe() NULL ptr dereference report by     Martin Bligh, I think Andrew has a few others he's tracked.. ]   		 
Yes. Anything that keeps a nice patch series that you can merge as a nice   patch series works fine.   For example, Al Viro used to use (still uses?) RCS to maintain his   work-in-progress. That worked fine, because he had a process where he   would just extract them as patches.   The reason CVS doesn't work well is partly because CVS just sucks at so   many levels, and because people start using it as more than a "series of   patches" repository. People might cherry-pick one or two changes from a   CVS, but it quickly becomes totally impossible to do anything sane at all,   or even to cherry-pick more than a few patches, because after a while   you've lost the ability to pick out individual changes.   Something like quilt works fine, because individual patches never get lost   in other patches (they might get merged with another patch on purpose, but   that's a separate issue). Anything that understands the notion of   changesets and can be taught to re-order them should be able to work the   same way.   So the important thing is to have _some_ proper linear changeset history,   preferably one where you can re-order them (so that if you cherry-pick a   set of changesets, you can mark them as having been merged, and keep the   _rest_ as a linear changeset history).   CVS just sucks. End of story. It works badly at so many levels that it's   just not even funny.   		 
I'd ask you and Oleg to re-synchronize, and perhaps Oleg to re-send the   (part of?) the series that has no debate. I'm unsure, for example, whether    I already applied #1, and it looks like there's agreement on #3 and #4,   but basically, just to make sure, can Oleg please re-send to make sure I   got it right?   Getting a screwed-up RCU thing is going to be too painful to debug, so I'd   rather get it right the first time it hits my tree..   		 
Note that in the case where the _latest_ state of the tre you are tracking   really matters, then doing a "git pull" is absolutely and unquestionably   the right thing to do.    So if people thought that I don't want to have sub-maintainers pulling   from my tree _at_all_, then that was a mis-communication. I don't in any   way require a linear history, and criss-cross merges are supported   perfectly well by git, and even encouraged in those situations.   After all, if tree B starts using features that are new to tree A, then   the merge from A-B is required for functionality, and the synchronization   is a fundamental part of the history of development. In that cases, the   history complexity of the resulting tree is a result of real development   complexity.   Now, obviously, for various reasons we want to avoid having those kinds of   linkages as much as possible. We like to have develpment of different   subsystems as independent as possible, not because it makes for a "more   readable history", but because it makes it a lot easier to debug - if we   have three independent features/development trees, they can be debugged   independently too, while any linkages inevitably also mean that any bugs   end up being interlinked..   		 
Can we do one final cleanup? Do all the magic in _one_ place, namely the   x86 Kconfig file.   Also, I don't think the NOHIGHMEM dependency is necessarily correct. A   2G/2G split can be advantageous with a 16GB setup (you'll have more room   for dentries etc), but you obviously want to have HIGHMEM for that..   Do it something like this:   	choice  		depends on EXPERIMENTAL  		prompt "Memory split"  		default DEFAULT_3G  		help  		  Select the wanted split between kernel and user memory.  		  If the address range available to the kernel is less than the  		  physical memory installed, the remaining memory will be available  		  as "high memory". Accessing high memory is a little more costly  		  than low memory, as it needs to be mapped into the kernel first.  		  Note that selecting anything but the default 3G/1G split will make  		  your kernel incompatible with binary only modules.   		config DEFAULT_3G  			bool "3G/1G user/kernel split"  		config DEFAULT_3G_OPT  			bool "3G/1G user/kernel split (for full 1G low memory)"  		config DEFAULT_2G  			bool "2G/2G user/kernel split"  		config DEFAULT_1G  			bool "1G/3G user/kernel split"  	endchoice   	config PAGE_OFFSET  		hex   and then asm-i386/page.h can just do   and you're done.   If you ever want to change the offsets, you're only changing the Kconfig   file, and as you can tell, the syntax is actually much _nicer_ that using   the C preprocessor, since these kinds of choices is exactly what the   Kconfig language is all about.   Please?   		 
Ok, pulled.   However, fixing up a trivial conflict in i386/Makefile, I noticed this:   				    echo "-mregparm=3"; fi ;)   and it strikes me that this is WRONG.   It's wrong for some subtle reasons: it means that CONFIG_REGPARM is set   whether or not it is actually _used_, which means that anybody who depends   on CONFIG_REGPARM in the sources is just screwed.   Now, for this particular usage, the only breakage is in the i386   asm/module.h, which will report "REGPARM" in MODULE_REGPARM regardless   of whether the kernel was compiled with -mregparm=3 or not. So it's mainly   cosmetic.   But it strikes me that we'd be a _lot_ better off if the Kconfig phase   would check the compiler version, instead of us checking it dynamically a   hundred times in the Makefiles. It would be more efficient, and we could   make things like this more _correct_.    Comments?   		 
Good point. I just took the naming from the original one. Especially if   all the logic is moved into Kconfig files, it has nothing to do with   DEFAULT what-so-ever. More of a VMSPLIT_3G or similar..   		 
Yes, except I think we need to make the "depends on" include !X86_PAE:   	depends on EXPERIMENTAL &amp;&amp; !X86_PAE   since PAE depends on the 3G/1G split (we could make it work for a pure   2G/2G split, but that's a separate issue, and then we'd need to change the   CONFIG_PAGE_OFFSET defaults to be something like   (but that's definitely not appropriate for now - that's a separate issue,   after somebody has verified that PAE and 2G:2G works)   Also, I think the arch/i386/mm/init.c snippet should just be removed. If   we make the split configurable, I don't see that we should warn about a   configuration where you have less memory than the point where the split   makes sense. A distribution (either something like Fedora _or_ just a   internal company "standard image") migth decide to use 2G:2G, but not all   machines might have lots of memory. Warning about it would be silly.   Anyway, this should go into -mm, and I'd rather have it stay there for a   while. I've got tons of stuff for 2.6.16 already, I'd prefer to not see   this kind of thing too..   		 
It's not really a linearization - at no time does git-bisect _order_ the   commits. After all, no linear order actually exists.    Instead, it really cuts the tree up into successively smaller parts.    Think of it as doing a binary search in a 2-dimensional surface - you   can't linearize the plane, but you can decide to test first one half of   the surface, and then depending on whether it was there, you can halve   that surface etc..    Yes. Although if you _know_ that the problem happened in a specific file   or specific subdirectory, you can actually tell "git bisect" to only   bother with changes to that file/directory/set-of-directories to speed up   the search.   IOW, if you absolutely know that it's ACPI-related, you can do something   like   	git bisect start drivers/acpi arch/i386/kernel/acpi   to tell the bisect code that it should totally ignore anything that   doesn't touch those two directories.   However, if it turns out that you were wrong (and the ACPI breakage was   brought on by something that changed something else), "git bisect" will   just get confused and report the wrong commit, so this is really something   you should be careful with (and verify the end result by checking that   undoing that _particular_ commit really fixes things).   And yes, "git bisect" _will_ work with bugs that depend on two branches of   a merge: it will point to the merge commit itself as being the problem.   Now, at that point you really are screwed, and you'll have to figure out   why both branches work, but the combination of them do not.   Maybe it's as simple as just a merge done wrong (bad manual fixups), but   maybe it's a perfectly executed merge that just happens to have one branch   changing the assumptions that the other branch depended on.   Happily, that is not very common. I know people are using "git bisect",   and I don't think anybody has ever reported it so far. It will happen   eventually, but I'd actually expect it to be much more common that "git   bisect" will hit other - worse - problems, like bugs that "come and go",   and that a simple bisection simply cannot find because they aren't   totally repeatable.   			 
Well, right now _all_ the non-3:1 cases need to be disbarred. I think we   depend on the kernel mapping only ever being the _one_ last entry in the   top-level page table, which is only true with the 3:1 mapping.   But I didn't check.   		 
No, that's not how bisect works at all.   It's true that if a commit is bad, then all the commits _reachable_ from   that commit are considered bad.    And it's true that if a commit is good, then all commits that _reach_ that   commit are considered good.   But that doesn't mean that there is an ordering. The commits that fall   into the camp of being "neither good nor bad" are _not_ ordered. There are   commits in there that are not directly reachable from the good commit.   Exactly.    And a git graph is not really a two-dimensional surface, but exactly was   with a 2-dimensional surface, it is _not_ enough to have a *point* to   separate the good from bad.   You need to have a _set of points_ to separate the good from the bad. You   can think of it as a line that bisects the surface: if you were to print   out the development graph, the set of points literally _do_ form a virtual   line across the development surface.   (Actually, you can't in general print out the development graph on a   2-dimensional paper without having development lines that cross each   other, but you could actually do it in three dimensions, where the   "boundary" between good and bad is actually a 2-dimensional surface in   3-dimensional space).   But to describe the surface of "known good", you actually just need a list   of known good commits, and the "commits reachable from those commits"   _becomes_ the surface.   No it is not. It's a very good comparison.   In a linearized model (one-dimensional, fully ordered set), the only thing   you need for bisection is two points: the beginning and the end.   In the git model, you need _many_ points to describe the area being   bisected. Exactly the same way as if you were to bisect a 2-dimensional   surface.   Now, the git history is _not_ really a two-dimensional surface, so it's   just an analogy, not an exact identity. But from a visualization   standpoint, it's a good way to think of each "git bisect" as adding a   _line_ on the surface rather than a point on a linear line.   Read the code.   I'm pretty proud of it. It's simple, and it's obvious once you think about   it, but it is pretty novel as far as I know. BK certainly had nothing   similar, not have I heard of anythign else that does it. Git _might_ be   the first thing that has ever done it, although it's simple enough that I   wouldn't be surprised if others have too.   			 
Actually, the way I think of it is akin to the "light cones" in physics. A   point in space-time doesn't define a fully ordered "before and after": but   it _does_ describe a "light cone" which tells you what is reachable from   that point, and what that point reaches. Within those cones, that   particular point ("commit") has a strict ordering.   And exactly as in physics, in git there's a lot of space that is _not_   ordered by that commit. And the way to bisect is basically to find the   right points in "git space" to create the right "light cone" that you   find the point where the git space that is reachable from that commit has   the same volume as the git space that isn't reachable.   And maybe that makes more sense to you (if you're into physics), or maybe   it makes less sense to you.   Now, since we always search the "git space" in the cone that is defined by   "reachable from the bad commit, but not reachable from any good commit",   the way we handle "bad" and "good" is actually not a mirror-image. If we   fine a new _bad_ commit, we know that it was reachable from the old bad   commit, and thus the old bad commit is now uninteresting: the new bad   commit forms a "past light cone" that is a strict subset of the old one,   so we can totally discard the old bad commit from any future   consideration. It doesn't tell us anything new.   In contrast, if we find a new _good_ commit, the "past light cone" (aka   "set of commits reachable from it") is -not- necessarily a proper superset   of the previous set of good commits, so when we find a good commit, we   still need to carry the _other_ good commits around, and the "known good"   universe is the _union_ of all the "good commit past lightcones".   Then the "unknown space" is the set difference of the "past lightcone of   the bad commit" and of this "union of past lightcones of good commits".   It's the space that is reachable from the known-bad commit, but not   reachable from any known-good commit.   So this means that when doing bisection, what we want to do is find the   point in git space that has _new_ "reachability" within that unknown space   that is as close to half that volume as space as possible. And that's   exactly what "git-rev-list --bisect" calculates.   So every time, we try to either move the "known bad" light-cone down in   time in the unknown space, _or_ we add a new "known good" light-cone. In   either case, the "unknown git space" keeps shrinking by half each time.   ("by half" is not exact, because git space is not only quanticized, it   also has a rather strange "distance function". In other words, we're   talking about a rather strange space. The good news is that the space is   small enough that we can just enumerate every quantum and simply   calculate the volume it defines in that space. IOW, we do a very   brute-force thing, and it works fine).   			 
Yes, that also works.   I think "quilt" is really the right thing here, although stg may be even   easier due to the more direct git integration. But with a smallish number   of patches, just doing patch management by hand is obviously simply not a   huge problem either, so extra tools may just end up confusing the issue.   		 
Correct. The branch head files really are just plain ascii references to   the top commit, you can rename branches by just renaming the file (and   thus switch branches by just cross-renaming them, ie just switching the   contents).   		 
I'm actually somewhat inclined to not pull any more. We've had lots of   (hopefully minor) issues for the last few days, and I know that people   had DRM issues with the -mm tree (which I assume tracked this tree) not   more than a week or so ago.   IOW, I want to make sure that my tree is somewhat stable again. I don't   want -rc1 to be horrible.   		 
This may be fixed by the current -git tree:   	commit 1bc691d3, Tejun Heo htejun@xxxxxxxxx:   	[PATCH] fix queue stalling while barrier sequencing   or if that isn't it, and you have an IDE drive, can you try if the   appended trivial patch makes a difference?   			 
I just pushed out a commit that reverts the IDE softirq request   completion, so if you pull a recent enough git tree, and you see that   revert (by Jens), the patch in the previous email won't apply, but it   won't be needed either.   		 
Btw, when doing the "From:" thing, I'd really prefer the email address   too, so that it ends up in the author field.   In fact, if there is nothing that looks like an email address, my patch   application scripts will ignore this "From:" in the body of the mail, and   use the From: from the headers (now, in this case it obviously is the   right thing to do, but if somebody else passes the email on to me, it   would result in you not being properly credited as author for the commit,   although the commit _message_ would then have that "From:" line in it).   Having an email address in the authorship data is just too convenient (ie   something goes wrong with the change, and we need to contact the author or   similar).   			 
Ok, since Andrew backs you up on that one, I'll merge asap.   		 
I think this is done wrong.   It should "select CRYPTO" rather than "depends on CRYPTO".   Otherwise people won't see it just because they don't have crypto enabled,   which is not very user-friendly.   Btw, why are these casts there? Either the functions are of the right   type, or they aren't. In neither case is the cast correct.   I do realize that there are comments in net/iw_handler.h that says that   you should do the cast, but that's no excuse for crap or stupid code.   If it's an issue of trying to make greppable code, why not have  just the comment?   Hmm?   		 
Hmm. If Roman dislikes it, he must dislike the fact that we already do   exactly this for a ton of different things. There's something like 2000+   "select" statements in the kernel Kconfig tree, and just grepping for   "select.*CRYPTO" gets 52 hits..   So this isn't new..   		 
Hmm.. I don't see any recent changes that could affect this. Not after   2.6.15, but in fact not even after 2.6.14.   Your oops is also interesting in another way...   This is a free'd page fault, so it's due to DEBUG_PAGEALLOC rather than a   wild pointer.   Is that something new for you? Maybe the bug is older, but you've enabled   PAGEALLOC only recently? Also, out of interest, have you enabled slab   debugging?   That said, the whole ubh_get_usb_second() and ubh_get_usb_third() thing is   pretty damn scary. There's no testing of the values passed in at all and   comparing them to the allocated buffer heads. But from what I can tell,   ubh_bread_uspi() will zero out any unallocated bh's, and it certainly   _looks_ like the calculations to calculate "usb2" should fit within the   sectors that were read..   Very strange.   			 
Ok, That explains why it started happening for you only _now_, but not why   it happens in the first place.   Can you test if the patch that Evgeniy sent out fixes it for you even with   PAGEALLOC debugging enabled?   Evgeniy - That is one ugly macro, can you (or Alexey, for that matter:   somebody who can test it) turn it into an inline function or something to   make it half-way readable? I realize that means changing the arguments too   (right now that horrid macro accesses "uspi" directly - uggghhh).   If somebody maintains - or is interested in doing so - UFS, please speak   up, we don't have anybody listed in the MAINTAINERS file, and when I look   through the history, all I see is updates due to secondary issues (ie   somebody did generic cleanups and just happened to touch UFS as part of   that, rather than working _directly_ on UFS issues).   		 
I was thinking of something even more abstracted:   		unsigned int offset)   and then just doing   		((struct ufs_super_block_first *)get_usb_offset(uspi, 0))   		((struct ufs_super_block_second *)get_usb_offset(uspi, UFS_SECTOR_SIZE))   		((struct ufs_super_block_third *)get_usb_offset(uspi, 2*UFS_SECTOR_SIZE))   or something similar. Which seems a hell of a lot more readable to me, and   assuming it passes testing (ie I didn't screw up), I think it's more   likely to stay correct in the future and just generally be maintainable.   Hmm?   		 
The whole point of the pre-batching was that apparently the non-batched   bootmem code took ages to boot in simulation with lots of memory. I think   it was the ia64 people who used simulation a lot. So..   		 
Yes.   Ok. Then I doubt anybody will complain. I'm still wondering if some of the   other ugliness was due to some simulator strangeness issues, but maybe   even ia64 doesn't care that much any more..   		 
This is a final warning.   If the next SCSI merge tries to come in the day before the release window   closes, I WILL SIMPLY IGNORE IT, and the damn thing can wait until next   release.   We discussed this last time around. You guys had SIX WEEKS of calming-down   time during the last tree to do your SCSI development. Instead, you're   apparently AGAIN trying to cram it into the two weeks when merges are   supposed to happen, and then going in under the radar the day before I'm   about to do a -rc1, so that we won't have any time at all to fix even   obvious problems, is not how it's supposed to work.   So as far as big SCSI merges (and I'm seriously considering this same   policy for networking too, because it worked so badly this time around) is   concerned, they'd better be ready in the _first_ week of the two-week   window, simply because I'm fed up with this last-minute thing.   The point of having a TWO WEEK merge window is not that you use two weeks   for development, and then merge the last possible chance. Not at all. The   point of giving people two weeks is so that people don't have to be   totally synchronized - somebody can be on vacation, some problem could   have cropped up that people wanted fixed first, etc etc.   So next time around: merge in the first week. And you can merge more than   once, so if there's something missing MERGE ANYWAY, for christ sake! Then,   if you have a small further feature the last day, at least it's _small_,   and most of the new stuff has been getting some testing.   I'm pissed off. No more of these "developer limbo dance" games. I thought   it was clear last time around.    And hell yes, I'm serious. The SCSI merge window next time is one week.   Not a day more. After that first week, you get an additional _smaller_   merge window of another week for smaller new features. Because I'm not   going to take this a third time in a row.   			 
Btw, before you even say something - my apologies.    You did do things right, and I merged a first SCSI merge on Jan 4th, and I   was wrong - there was no "everything on the last day" event this time   around.   Sorry. Mea culpa.   		 
Ok, it's two weeks since 2.6.15, and the merge window is closed.   In fact, it already closed yesterday when I was planning on doing the   release, but we had people over for dinner, and things devolved. "The   best-laid plans of mice and kernel-developers.."   Anyway, it's out there now. The ShortLog is pretty readable - if you are   into that kind of stuff - but as usual for an -rc1 release (which has all   the frantic merging going on), it's actually too big to post on the kernel   list due to the size limits. It's weighs in at 4000+ lines and 169kB.   Anyway, if you're a git user, here's what generated the shortlog:   	git-rev-list --no-merges --pretty=short v2.6.15..v2.6.16-rc1 |   		git-shortlog  ../ShortLog   and you can find the _full_ log on kernel.org as ChangeLog-2.6.16-rc1 if   you want to.   The diffstat is also too big to post. It's also all over the map: there's   actually a fair number of cleanups in there that have affected a lot of   files, as did the new mutexes, for example.    There's also updates for various architectures: powerpc continues the long   slog to a merged tree - now ppc32 is mostly done too, but there's updates   to arm, x86[-64], m68k, frv, ia64, pa-risc, m32r, s390, etc..   And usb, i2c, pcmcia, fbdev, v4l, drm, scsi, alsa, input layer, network   drivers, infiniband.. The tty layer got some clean-ups too (wonder of   wonders).   OCFS2 was merged, fuse updates, fat fixes, and p9fs, XFS and NFS updates.   And largish networking updates: there's a "common netfilter" setup now,   which you'll notice when you do "make config" or equivalent, since a lot   of the netfilter rules now work on ipv4 and/or ipv6 rather than having   separate (and duplicate) versions for each.   			 
Well, I'd rather not do it in the source control management itself, simply   because people are notoriously bad at deciding what is "important".   It goes something like this: "By definition, anything _you_ work for is   crap and unimportant, while _my_ work is the most important thing ever,   even if it happens to be just fixing typos".   Yeah, that's a bit over-generalized, but it definitely has a kernel of   truth to it. Also, it sometimes turns out that something nobody ever   really thought about turns out to have tons of side effects and needs lots   of fixing.   So asking developers to rate how important their work is just doesn't   really work.    On the other hand, maybe we could have something where people could easily   send hints - as they are merged - about new things, just to help. Also,   we do have automation that can help.   For example, one thing that git does well is that almost all tools can   follow not just a particular file, but a whole subdirectory (or a set of   subdirectories). So what _I_ did when I looked at the shortlog and   realized that it's huge, but I wanted to give something of a view of what   changed, was to do   	git log v2.6.15..v2.6.16-rc1 -- fs/ |  		git-shortlog |  		less -S   which restricts the log to just things that changed in the fs/   subdirectory. That allows you to look at more focused logs, which makes it   easier to dig into a particular feature or area.   [ Side note: you don't have to have just one directory you track: if     you're only interested in a certain set of areas, you can ask for     several specific subdirectories or files:   	git log v2.6.15..v2.6.16-rc1 -- fs/ext3/ fs/xfs/     will give the log only for stuff that changed either of those two     directories ]   Also, if you want to judge how big a patch is by the number of files it   changed, that's easy enough to do too:   	git-rev-list v2.6.15..v2.6.16-rc1 |  		while read id  		do  			files=$(git-diff-tree -r --name-only $id | wc -l)  			echo -e $id $files  		done |  		sort -k2 -n -r |   		git-diff-tree --pretty --stdin -s |  		less -S   which admittedly takes a bit of time, but will give you a "log" of every   single commit in the 2.6.15..2.6.16-rc1 range, sorted by how many files it   touches (most files first).   Now, admittedly, "number of files touched" is not a very good   approximation of importance, but it can still be interesting, and it may   be a good approximation for "how invasive was the change", in the sense   that it is a real measure of how likely a commit was to impact other   people.   For example, in this case, the #1 commit is 2e4e6a17:       [NETFILTER] x_tables: Abstraction layer for {ip,ip6,arp}_tables   which actually is one of the more important ones. The other top ones are   (#2..#10):       [PATCH] USB: remove .owner field from struct usb_driver      [PATCH] mutex subsystem, semaphore to mutex: VFS, -i_sem      [PATCH] TTY layer buffering revamp      [PATCH] I2C: Remove .owner setting from i2c_driver as it's no longer needed      [PATCH] i2c: Drop i2c_driver.flags, 2 of 3      [INET_SOCK]: Move struct inet_sock &amp; helper functions to net/inet_sock.h      [ARM] 3260/1: remove phys_ram from struct machine_desc (part 2)      V4L/DVB (3344a): Conversions from kmalloc+memset to k(z|c)alloc      [PATCH] powerpc: sanitize header files for user space includes   some of which are very core (the TTY layer buffering revamp), others are   more pedestrian and just happen to change a lot.   Btw: a word of warning - git is efficient, but doing things like the above   does require a bit of computing power. The above pipeline to generate the   log sorted by number of files changed takes just over a minute to execute   on a 2.5GHz dual G5 box. I'd also suggest you do this on a tree that you   have recently re-packed, just to avoid the expense of opening millions of   small files.   Now, if you save off the ordered list of commit IDs to a file:   	git-rev-list v2.6.15..v2.6.16-rc1 |  		while read id  		do  			files=$(git-diff-tree -r --name-only $id | wc -l)  			echo -e $id $files  		done |  		sort -k2 -n -r  most-invasive-commits   you can do other tricks with git too:   	head -25 most-invasive-commits |  		git-diff-tree --stdin --pretty -s |   		git-shortlog |  		less -S   will do a shortlog that contains just the 25 most invasive commits.   Is this useful to you? I dunno.  I thought I'd spread the git gospel and   see if somebody gives me a "Halleluja!"   		 
Avoid 4.0.0 and 4.0.1, but other than that, I don't think there are really   any huge mistakes to be made. You'll be very hard-pressed to even find any   of the old buggy early-3.x series compilers.   		 
Why?   The real downside is that "atomic_inc_nonzero()" is a lot more expensive   than checking for zero on x86 (and x86-64).   The reason it's offset is that on architectures that automatically test   the _result_ of an atomic op (ie x86[-64]), it's easy to see when   something _becomes_ negative or _becomes_ zero, and that's what   	atomic_add_negative  	atomic_inc_and_test   are optimized for (there's also "atomic_dec_and_test()" which reacts on   the count becoming zero, but that doesn't have a pairing: there's no way   to react to the count becoming one for the increment operation, so the   "atomic_dec_and_test()" is used for things where zero means "free it").   Nothing else can be done that fast on x86. Everything else requires an   insane "load, update, cmpxchg" sequence.   So I disagree with this patch series. It has real downsides. There's a   reason we have the offset.   I suspect that whatever "nice optimizations" you have are quite doable   without doing this count pessimization.   		 
Irrelevant. If "atomic_add_unless()" is in a hot path and inlined, we're   doing something else wrong anyway. It's not a good op to use. Just think   of it as being very expensive.   The _only_ user of "atomic_add_unless()" is "dec_and_lock()", which isn't   even inlined. The fact that gcc ends up "unrolling" the loop once is just   fine.   Please keep it that way.    		 
And I'm not applying it. I'd be crazy to replace good code by code that is   objectively _worse_.   The fact that you _document_ that it's worse doesn't make it any better.   The places that you improve (in the other patches) seem to have nothing at   all to do with the counter skew issue, so I don't see the point.   So let me repeat: WHY DID YOU MAKE THE CODE WORSE?   		 
One side note on your patch: the pure bit _test_ operation is very cheap,   but the "bit change" operation is very expensive (and not really any less   expensive than the "test-and-change" one).   So the patch to avoid "test_and_clear_bit()" really helps only if the test   usually results in us not doing the clear. Is that the case? Hmm..   So I _think_ that at least the case in "isolate_lru_page()", you'd   actually be better off doing the "test-and-clear" instead of separate   "test" and "clear-bit" ops, no? In that one, it would seem that 99+% of   the time, the bit is set (because we tested it just before getting the   lock).   No?   Yes.   Now, that whole "we might touch the page count" thing does actually worry   me a bit. The locking rules are subtle (but they -seem- safe: before we   actually really put the page on the free-list in the freeing path, we'll   have locked the LRU list if it was on one).   But if you were to change _that_ one to a   I think that would be a real cleanup. And at that point I won't even   complain that "atomic_inc_test()" is faster - that "get_page_testone()"   thing is just fundamentally a bit scary, so I'd applaud it regardless.   (The difference: the "counter skewing" may be unexpected, but it's just a   simple trick. In contrast, the "touch the count after the page may be   already in the freeing stage" is a scary subtle thing. Even if I can't   see any actual bug in it, it just worries me in a way that offsetting a   counter by one does not..)   		 
The thing I minded was the _other_ changes, namely the de-skewing itself.   It seemed totally unnecessary to what you claimed was the point of the   patch.   So I objected to the patch on the grounds that it did what you claimed   badly. All the _optimization_ was totally independent of that de-skewing,   and the de-skewing was a potential un-optimization.   But if you do the optimizations as one independent set of patches, and   _then_ do the counter thing as a "simplify logic" patch, I don't see that   as a problem.   Side note: I may be crazy, but for me when merging, one of the biggest   things is "does this pass my 'makes sense' detector". I look less at the   end result, than I actually look at the _change_. See?   That's why two separate patches that do the same thing as one combined   patch may make sense, even if the _combined_ one does not (it could go the   other way too, obviously).   		 
Heh. Now you keep the count offset, but you also end up removing all the   comments about it (still) being -1 for free.    And your changelog talks about "atomic_inc_not_zero()" even though the   code actually does   which makes it pretty confusing ;)   I also think it's wrong - you've changed put_page_testzero() to use   "atomic_dec_and_test()", even though the count is based on -1.   So this patch _only_ works together with the next one, and is invalid in   many ways on its own. You should re-split the de-skew part correctly..   		 
ACK, looks ok by me now.   I assume this is still "-mm" material for now.   		 
Interestingly, __alignof__(unsigned long long) is 8 these days, even   though I think historically on x86 it was 4. Is this perhaps different in   gcc-3 and gcc-4?   Or do I just remember wrong?   		 
Bzzt. Look closer.   The Linux kernel has _always_ been under the GPL v2. Nothing else has ever   been valid.   The "version 2 of the License, or (at your option) any later version"   language in the GPL copying file is not - and has never been - part of the   actual License itself. It's part of the _explanatory_ text that talks   about how to apply the license to your program, and it says that _if_ you   want to accept any later versions of the GPL, you can state so in your   source code.   The Linux kernel has never stated that in general. Some authors have   chosen to use the suggested FSF boilerplate (including the "any later   version" language), but the kernel in general never has.   In other words: the _default_ license strategy is always just the   particular version of the GPL that accompanies a project. If you want to   license a program under _any_ later version of the GPL, you have to state   so explicitly. Linux never did.   So: the extra blurb at the top of the COPYING file in the kernel source   tree was added not to _change_ the license, but to _clarify_ these points   so that there wouldn't be any confusion.    The Linux kernel is under the GPL version 2. Not anything else. Some   individual files are licenceable under v3, but not the kernel in general.    And quite frankly, I don't see that changing. I think it's insane to   require people to make their private signing keys available, for example.   I wouldn't do it. So I don't think the GPL v3 conversion is going to   happen for the kernel, since I personally don't want to convert any of my   code.    No. You think "v2 or later" is the default. It's not. The _default_ is to   not allow conversion.   Conversion isn't going to happen.   		 
I wonder if the right fix wouldn't be to free the user struct early,   instead of freeing it from RCU. Hmm?   		 
No. We actually have a lot of code that is more widely licensed than just   GPLv2. There's the GPL/BSD code, and there's a lot of files that have the   ".. or any later version" addendum which means that they are GPLv3   compatible.   The only thing that the kernel requires is that since the majority of the   code is actually GPLv2-only, that in order for you to be able to link with   the code, your license has to be GPLv2-compatible.   A "GPLv3 _only_" license is not compatible with GPLv2, since v3 adds new   limitations to re-distribution. But what you can do is to dual-license the   code - the same way we've had GPL/BSD dual licenses. Of course, that   effectively becomes the same as "GPLv2" with the "any later version"   clause, but if you like the v3 in _particular_, you can actually mention   it specifically (ie you can dual-license under "v2 _or_ v3", but without   the "any later version" wording if you want).   Note that the Linux kernel has had the clarification that the "by default,   we're version-2 _only_" for a long time, and that limitation is not a new   thing.   You can argue that I should have made that clear on "Day 1" (back in 1992,   when the original switch to the GPL happened), but the fact is, all of the   development for the last five or more years has been done with that "v2   only, unless otherwise stated" (I forget exactly when it happened, but it   was before we even started using BK, so it's a loong time ago).   Also, this has been discussed before, and anybody who felt that they   didn't want to have the "v2 only" limitation has been told to add the "or   any later version" thing to their own code, so nobody can claim that I   restricted their licensing.    So to recap:    - Linux has been v2-only for a _loong_ time, long before there was even      any choice of licenses. That explicit "v2 only" thing was there at      least for 2.4.0, which is more than five years ago. So this is not some      sudden reaction to the current release of GPLv3. This has been there      quite _independently_ of the current GPLv3 discussion.    - if you disagree with code you write, you can (and always have been      able) to say so, and dual-license in many different ways, including      using the "or later version" language. But that doesn't change the fact      that others (a _lot_ of others) have been very much aware of the "v2      only" rule for the kernel, and that most of the Linux kernel sources      are under that rule.    - People argue that Linux hasn't specified a version, and that by virtue      of paragraph 9, you'd be able to choose any version you like. I      disagree. Linux has always specified the version: I don't put the      license in the source code, the source code just says   	Copyright (C) 1991-2002 Linux Torvalds      and the license is in the COPYING file, which has ALWAYS been v2. Even      before (for clarification reasons) it explicitly said so.      In other words, that "if no version is mentioned" simply isn't even an      argument. That's like arguing that "if no license is mentioned, it's      under any license you want", which is crap. If no license is mentioned,      you don't have any license at all to use it. The license AND VERSION      has always been very much explicit: linux/COPYING has been there since      1992, and it's been the _version_2_ of the license since day 1.      People can argue against that any way they like. In the end, the only      way you can _really_ argue against it is in court. Last I saw,      intentions mattered more than any legalistic sophistry. The fact that      Linux has been distributed with a specific version of the GPL is a big      damn clue, and the fact that I have made my intentions very clear over      several years is another HUGE clue.    - I don't see any real upsides to GPLv3, and I do see potential      downsides. Things that have been valid under v2 are no longer valid      under v3, so changing the license has real downsides.   Quite frankly, _if_ we ever change to GPLv3, it's going to be because   somebody convinces me and other copyright holders to add the "or any later   license" to all files, just because v3 really is so much better. It   doesn't seem likely, but hey, if somebody shows that the GPLv2 is   unconsitutional (hah!), maybe something like that happens.    So I'm not _entirely_ dismissing an upgrade, but quite frankly, to upgrade   would be a huge issue. Not just I, but others that have worked on Linux   over the last five to ten years would have to agree on it. In contrast,   staying with GPLv2 is a no-brainer: we've used it for almost 15 years, and   it's worked fine, and nobody needs any convincing.   And that really is a big issue: GPLv2 is a perfectly fine license. It has   worked well for us for fourteen years, nothing really changed with the   introduction of GPLv3. The fact that there is a newer license to choose   from doesn't detract from the older ones.   			 
Indeed. The GPL (both v2 and v3) disallow restricting usage.    So certain _code_ can be either v2 or v3 only, but you can't make that   decision based on how the code is used.    So you can't license, for example, your code "udner the GPL only for the   Linux kernel". Trust me, some companies have actually wanted to do exactly   that - they wanted to distribute their code, but _only_ for the kernel   (and you'd not be allowed to use it for any other GPL'd project). That   just cannot fly. It's either GPL, or it isn't. There's no "GPL with the   following rules".   		 
You very much _can_ dual-license it, and say "for the kernel, we license   it under GPLv2".   But if you do that, then the GPLv2 licensing in the kernel means that   somebody else can take it from the kernel, and ti can still be under the   GPLv2 too (_and_ the GPLv3, for that matter - the kernel GPLv2 license in   no way takes away the ability to use it in any other way that you've   dual-licensed it).   In other words, you cannot say "outside of the kernel, you ahve to use the   GPLv3", because the GPLv2 usage _inside_ of the kernel requires that the   GPLv2 also be usable outside of it.   But you most definitely _can_ dual-license in general. It's perfectly fine   to license something under the GPLv2 and allow redistribution with the   kernel, _and_ have the same code as part of some other project under   GPLv3.   It's just that you cannot limit the kernel version to just a kernel   project. Somebody can (at any time) take the Linux kernel, and turning it   into some totally other GPL-v2-licensed project.   (This really is not at all specific to the GPLv3 - the same thing holds   for any other dual licensing with the GPL. You  cannot license your   proprietary code to be "GPL only within the kernel, and proprietary   anywhere else". The GPL inherently requires that the code can be used in   another project).   		 
Sorry, I think you're wrong.   We've _always_ said which license explicitly. It's in the COPYING file.   Even before the clarification, the COPYING file has always said                        GNU GENERAL PUBLIC LICENSE                         Version 2, June 1991    Copyright (C) 1989, 1991 Free Software Foundation, Inc.                         51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA   Everyone is permitted to copy and distribute verbatim copies   at the very top.   How can you say that we didn't specify a version?   If you distribute a program, and you just say "I license this under the   GPL", THEN you don't specify a verion.   But if you distribute a program, and the ONLY license that is associated   with it is a specific version of a license file, then that's what you   have, UNLESS SOMETHING SAYS OTHERWISE.   This is basic copyright law, btw, and has nothing to do with the GPL per   se. If you don't have a license, you don't have any copyright AT ALL.   Linux kernel files don't say "This is licensed under the GPL". Not mine,   at least. I don't see the point, and I never have. There's a COPYING file   that specifies what the license is, and that COPYING file very much   specifies a very _specific_ version of the GPL. Always has.   		 
Yes.   IF that is the legal interpretation, then yes, I'd agree with you. And no,   I'm not a lawyer. However, the way I read it, it's not about just not   being able to hide the object code - it's fundamentally about being able   to replace and run the object code.   I may indeed be reading it wrong, but I don't think I am. It explicitly   says "install and/or execute".   So I think it says that if I have a private signing key that "enables" the   kernel on some hardware of mine, GPLv3 requires that private key to be   made available for that hardware. Note how that is tied to the _hardware_   (or platform - usualyl the checking would be done by firmware, of course),   not the actual source code of the program.   And that's really what I don't like. I believe that a software license   should cover the software it licenses, not how it is used or abused - even   if you happen to disagree with certain types of abuse.   I believe that hardware that limits what their users can do will die just   becuase being user-unfriendly is not a way to do successful business. Yes,   I'm a damned blue-eyed optimist, but I'd rather be blue-eyed than consider   all uses of security technology to necessarily always be bad.   		 
The GPLv2 is explicitly written so that _nothing_ else than a GPLv2   license can be compatible with it.   At most you can dual-license and effectively allow extra rights that way,   but the point is that the GPLv2 is always the limit for any restrictions   (and it's written so that anybody can always take any but the GPLv2   freedoms away - so if you dual-license, your licensees can always decide   to _only_ honor the GPLv2, and ignore any other license).   That's why, in order to re-license anythingt from GPLv2 to anything else,   the license grant itself must make it clear that the "anything else" was   always permissible in the first place. Which is why the FSF had only two   "outs": either people explicitly do the "..or any later version" thing,   _or_ people don't mention the version of the GPL at all, in which case any   version will do.   		 
This is really important, btw.   Yes, when we speak colloquially we talk about the fact that Linux is   licensed "under the GPL", but that is _not_ how anybody actually has ever   gotten a license legally. The ONLY way anybody has ever legally licensed   Linux is either with the original very strict copyright _or_ thanks to the   COPYING file. Nothing else really matters.    So the version of the GPL has always been explicit. At no point has the   kernel been distributed without a specific version being clearly mentioned   in the ONLY PLACE that gave you rights to copy the kernel in the first   place. So either you knew it was GPLv2, or you didn't have the right to   copy it in the first place.   In other words, Linux has _never_ been licensed under anything but the GPL   v2, and nobody has _ever_ gotten a legal Linux source distribution with   anything but a complete copy of GPLv2 license file.   So when I say that the additions at the top of the COPYING file are   nothing but clarifications, I'm not just making that up. Anybody who   claims that any Linux kernel I've ever made has ever been licensed under   anything else than those exact two licenses is just not correct.   And Alan, I know we've had this discussion before. You've claimed before   that my clarifications are somehow "changing" the license, and I've told   you before that no, they don't change the license, they just clarify   things that people keep on gettign wrong, or keep on being nervous about.   So people can argue all they want about this. But unless you get a real   legal opinion (not just any random shyster - a real judge making a   statement, or a respected professional who states his firm legal opinion   in no uncertain terms), I don't think you have a legal leg to stand on.   But no, IANAL. I'd be willing to bet real money that a real lawyer would   back me up on this, though.   			 
Well, the good news is that I was the only real copyright holder back then   (there's a couple of other people who contributed to 0.11 and/or 0.12,   mainly Ted T'so with the BSD terminal control stuff - ^Z and friends).   I used to even re-write patches to suit my style (this was back then, the   patches were smaller, and I was younger and had more energy). So some   things that others sent in patches for (I think Peter McDonald did pty's)   I ended up re-writing myself (and in the process I mixed up the master and   slave pty major number, iirc ;)   I think you can take it for granted that the GPL re-licensing was   retro-active. I'm the sole copyright holder for 99% of it, and there were   no objections to the relicensing even back when it happened, so I can   pretty much guarantee that there would be none now ;)   It was a kind of strange license. I didn't spend a whole lot of time   thinking about it ;)   		 
