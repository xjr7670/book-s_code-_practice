When faced with sensory information, human beings naturally want to find patterns to explain, differentiate, categorize, and predict. This process of looking for patterns all around us is a fundamental human activity, and the human brain is quite good at it. With this skill, our ancient ancestors became better at hunting, gathering, cooking, and organizing. It is no wonder that pattern recognition and pattern prediction were some of the first tasks humans set out to computerize, and this desire continues in earnest today. Depending on the goals of a given project, finding patterns in data using computers nowadays involve database systems, artificial intelligence, statistics, information retrieval, computer vision, and any number of other various subfields of computer science, information systems, mathematics, or business, just to name a few. No matter what we call this activity - knowledge discovery in databases, data mining, data science - its primary mission is always to find interesting patterns. Despite this humble-sounding mission, data mining has existed for long enough and has built up enough variation in how it is implemented, that has now become a large and complicated field to master. We can think of a cooking school, where every beginning chef is first taught how to boil water and how to use a knife before moving to more advanced skills like making a puff pastry or deboning a raw chicken. In data mining, we also have common techniques that even beginning data miners will learn: how to build a classifier or how to find clusters in data. The aim of this "mastering" level book, however, is to teach you some of the techniques you may not have seen yet in earlier data mining projects. In this first chapter, we will learn: What is data mining? We will situate data mining in the growing field of other similar concepts, and we will learn a bit about the history of how this discipline has grown and changed. How do we do data mining? Here we compare several processes or methodologies commonly used in data mining projects
In our data mining toolbox, measuring the frequency of a pattern is a critical task. In some cases, more frequently occurring patterns may end up being more important patterns. If we can find frequently occurring pairs of items, or triples of items, those may be even more interesting. In this chapter we begin our exploration of frequent itemsets, and then we extend those to a type of pattern called association rules. We will learn: What is a frequent itemset? What are the techniques for finding frequent itemsets? Where are the bottlenecks and how can we speed up the process? How can we extend a frequent itemset to become an association rule? What makes a good association rule? We will learn to describe the value of a particular association rule, given its level of support in the database, our confidence in the rule itself, and the value added by the rule we found. To do this, we will write a program to find frequent itemsets in an open data set of metadata (facts) about a group of software projects. Then we will learn to find frequent itemsets among the tags used to describe those projects. Next, we will learn how to extend a frequent itemset into an association rule by calculating its support in the database and then adding a probabilistic direction (X implies Y) confidence interval. Finally, we will learn how to interpret an association rule. Specifically, we want to understand what an association rule shows, and what does it not show?
In my set of outdoor tools, I have a large hand axe that I have always called a mattock. But my friend from the western United States calls it a Pulaski. When he asks me to hand him the Pulaski, it always gives me a moment of pause. Sometimes, we might know a thing by more than one name, or two things might share the same name, which can lead to confusion. This happens with people all the time. Have you ever been mistaken for someone else who shares your same first and last name? Have you ever used a nickname or an alias? In a children's playground, ten women might turn around when they hear a child call out "Mom!" A man who always goes by "Bob" would be immediately suspicious when an unfamiliar telephone caller asks to speak with "Robert". A pharmacy technician gives "John T. Smith" the medicine intended for "John M. Smith", leading to disastrous results. In this chapter, we are concerned with the accurate identification of entities, or things, and the correct assignment of matching entities. Are these two things really the same, or are they different? Can we determine that two similar-looking things really are the same, based on their other characteristics? To extend the market basket example from Chapter 2, suppose we are a grocery store, and we have our own database full of information about our shoppers and items they have purchased from our store. Now, imagine our store merges with another grocery store chain, and we are now the caretakers of all of their shopping data as well. We would like to see if any of our shoppers also shopped at this other store. But how do we connect the first data set to the second? Typically, we will look for some unique identifier that is in common between the two data sets. But what if that identifier does not exist? Or what if there is an attribute that both data sets have in common, but its values are not unique? How can we solve this challenge of entity matching where there is no clear, unambiguous connection between two data sets? In this chapter we will learn: Common strategies for entity matching, including attribute matching, disjoint sets, contextual matching, and profiling a typical match. How to evaluate efficacy of the chosen methods, including calculating the precision, recall, and F-measure for a result set. How to apply entity matching methods to a real-world problem using data from two separate collections of data about free, libre, and open source software (FLOSS) projects. Before we get started, you may be wondering where in the data mining workflow entity matching fits. Is it data cleaning, or data integration, or data analysis, or what? Typically, entity matching would be considered primarily a data cleaning and data integration step, since its main purpose is to produce a data set that is as accurate as possible, and one that is ready to be mined for other patterns. However, because we will use pattern-finding techniques in order to accomplish the matching, you will probably notice that it does have some of the flavor of a data analysis step as well.
Humans are very social creatures, and our ability to find connections - with each other and with other things in our lives - is one of our strongest impulses. We naturally love to connect with others, we distinguish our connections with different names or levels (friend, spouse, acquaintance, lover, enemy, bff, frenemy, boss, employee, stranger, co-worker, neighbor), and we sometimes persist these connections for years or decades. We are fascinated by seeing people from our network appearing in other, seemingly unconnected networks. We love the notion that there might be only six (or four, or three) degrees of separation between any two people on Earth. The "small world" phenomenon reminds us that we are more closely connected to each other than it may appear. So far in this book we have experimented a lot with finding connections between things, first by finding items that are commonly associated, and then by finding entities that appear different but are really the same. In this chapter, we will continue to explore how things are connected, but we will focus on data that can be represented as a network. We will learn: The basics of network theory, including how to measure a network, why the different shapes that we find in a network are interesting, how to organize some real world data into a structure that will let us analyze it as a network, how to find and interpret patterns in this real world data using Python and NetworkX, how to compare multiple versions of a network to see how the network has changed over time.
One of the most powerful skills we can master in data mining is learning how to deal with large amounts of unstructured or semi-structured textual data. Textual data, sometimes just called text, is important because it is everywhere, and because it conveys so much detail about the human experience in so many formats: books, news media, journals, government reports, case law, email messages, chat logs, product reviews, and so on. We also find text data in places we might not expect. For example, when the spoken word is written down it also becomes text, as do song lyrics and video transcripts. When we look at the code that makes up web pages and computer programs, we find text. When we train computers leave a record of what they have done, we find text logs. When we need a common, universally interoperable medium for communicating between devices we often use plain text to do so. Over the next few chapters, we will be exploring some of the ways that we can find patterns in text data, and in particular, we will look for patterns in natural human language text. First, in this chapter, we will learn how to detect the opinions or sentiments expressed in a text. This task, called sentiment analysis, helps us understand texts better by discerning the mood or tone of the human who wrote the text. We will learn: What sentiment analysis is, and why we might care about it, How to understand some of the most common techniques for finding the sentiment in a text, and what software tools are available to implement these techniques, and How to apply sentiment analysis to two real-world collections of text.
The next text mining tool we are going to add to our toolbox is actually from the domain of information extraction. When we talk about information extraction, we typically mean text mining techniques that use natural language processing to pull out key pieces of desired information from a large amount of unstructured text. I like to think of information extraction as being like a gold miner's sifting pan. Using these tools, we extract only the good stuff - the gold nuggets - and let the rest of the dirt fall away. In this chapter, the gold nuggets we will be sifting for are called named entities. Given a semi-structured or unstructured body of text, can we locate and extract all the named entities, such as people, places, or organizations, and leave the rest of the text behind? In this chapter we will learn: What named entities are and why they are useful to search for, What are the different techniques for finding named entities, and what are the benefits of each, How to find the named entities in a bunch of text, including how to differentiate them from other tagged parts of speech, How to apply these named entity recognition techniques to a real world problem, and How to determine whether our named entity recognition was successful or not.
In an era of information overload, the objective of text summarization is to write a program that can reduce the size of a text while preserving the main points of its meaning. The task is somewhat similar to the way an architect might create a scale model of a building. The scale model gives the viewer a sense of the important parts about the structure, but does so with a smaller size footprint, fewer details, and without the same expense in time or materials. Consider Reddit, a news-oriented social media site, with its thousands of news articles posted daily by users. Is it possible to generate a summary of a news article that is shorter but still conveys the key facts and general meaning of the original story? A few Reddit users created summary bots to do exactly this. These so-called TLDR bots (too long; didn't read) post summaries of user-submitted news stories, usually including a link to the original story and statistics to show by what percentage they reduced the text. One of these bots is named autotldr. Created in 2011, autotldr follows links to news stories and summarizes them in a comment posting. It always announces itself before its summary like this, "This is the best tl;dr I could make, original reduced by 73%. (I'm a bot)". Users seem to enjoy the autotldr bot, and its machine-generated news summaries have been upvoted 190,000 times. So how does this kind of text summarization actually work? In this chapter we will learn: What automatic text summarization is and why it is important, How to build a naive text summarization system from scratch, How to implement more sophisticated text summarizers and compare their effectiveness, How to combine the results of text summarizers to produce a composite of results.
Topic modeling in text is loosely related to the summarization techniques we explored in Chapter 7, but usually involves more complex techniques that produce a different type of result. While the goal of text summarization is to produce a version of a text that is reduced but still expresses common themes or concepts in a text, the goal of topic modeling is to expose the underlying concepts themselves. To extend our Chapter 7 metaphor where text summarization was similar to building a scale model of a house, here we can imagine topic modeling as the job of describing the purpose of a house based on the description of it. For example, the topic model of one house might be "busy family, storage space, low maintenance" and another could be "entertaining, luxury, showplace". These two models clearly represent two different houses, designed and built with two different purposes. A basic topic model for text could resemble a simple set of extracted keywords, such as those we used in Chapter 7, but in this chapter we want to go a bit deeper, looking at more sophisticated ways to develop themes from text. How does topic modeling work? In this chapter we will learn: What is topic modeling? What are some of the common techniques we can use to accomplish this task? What are the currently available libraries and tools for applying topic modeling in Python, and how do they work? How can we compare the effectiveness of a topic modeling approach, in terms of the results it generates? How to apply topic modeling to a real-world problem.